---
title: 浅入浅出强化学习
date: 2025-12-10 18:31  
tags: [强化学习, AI]
---

# 什么是强化学习

强化学习（Reinforcement Learning, RL）是一种机器学习方法，通过与环境的交互来学习如何采取行动以最大化累积奖励，让计算机实现从一开始什么都不懂，脑袋里没有一点想法, 通过不断地尝试, 从错误中学习, 最后找到规律, 学会了达到目的的方法. 这就是一个完整的强化学习过程。比如特斯拉的fsd自动驾驶就使用了部分强化学习用来训练。

![image2024-7-11_20-13-45](https://s41.ax1x.com/2025/12/10/pZubmdA.png)

强化学习需要一位虚拟的评委, 他不会告诉你如何移动, 如何做决定, 他为你做的事只有给你的行为打分, 那要怎么样只从分数中学习到我应该怎样做决定呢? 很简单, 我只需要记住那些高分, 低分对应的行为, 下次用同样的行为拿高分, 并避免低分的行为.

评委会根据开心程度来打分, 开心时, 可以得到高分, 不开心时得到低分. 有了这些被打分的经验, 就能判断为了拿到高分, 应该选择一张开心的脸, 避免选到伤心的脸. 这也是强化学习的核心思想. 

可以看出在强化学习中, 行为的分数是十分重要的. 所以强化学习具有分数导向性. 我们换一个角度来思考.这种分数导向性好比我们在监督学习中的正确标签.

![image2024-7-11_20-22-2](https://s41.ax1x.com/2025/12/10/pZubMJP.png)

# 强化学习的方法

强化学习包含了很多种算法, 比如有通过行为的价值来选取特定行为的方法, 包括使用表格学习的 q learning, 使用神经网络学习的DQN（ deep q network）, 还有直接输出行为的 policy gradients, 又或者了解所处的环境, 想象出一个虚拟的环境并从虚拟的环境中学习 等等.

![image2024-7-11_20-23-33](https://s41.ax1x.com/2025/12/10/pZub3QS.png)

# 什么是 Q - Leaning（小入门）

我们做事情都会有一个自己的行为准则, 比如小时候爸妈常说"不写完作业就不准看电视". 

所以我们在 写作业的这种状态下, 好的行为就是继续写作业, 直到写完它, 我们还可以得到奖励, 不好的行为 就是没写完就跑去看电视了, 被爸妈发现, 后果很严重. 

小时候这种事情做多了, 也就变成我们的记忆. Q learning 也是一个决策过程, 和小时候的这种情况差不多.

假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视, 所以现在我们有两种选择:

1. 继续写作业
2. 跑去看电视

因为以前没有被罚过, 所以我选看电视, 然后现在的状态变成了看电视, 我又选了 继续看电视, 接着我还是看电视, 最后爸妈回家, 发现我没写完作业就去看电视了, 狠狠地惩罚了我一次, 我也深刻地记下了这一次经历, 并在我的脑海中将 "没写完作业就看电视" 这种行为更改为负面行为, 我们在看看 Q learning 根据很多这样的经历是如何来决策的吧.

## Q-Learning 决策

![image2024-7-11_21-13-46](https://s41.ax1x.com/2025/12/10/pZub8sg.png)

假设我们的行为准则已经学习好了, 现在我们处于状态s1

我在写作业, 我有两个行为 a1, a2, 分别是看电视和写作业

根据我的经验, 在这种 s1 状态下, a2 写作业 带来的潜在奖励要比 a1 看电视高

这里的潜在奖励我们可以用一个有关于 s 和 a 的 Q 表格代替, 在我的记忆Q表格中, Q(s1, a1)=-2 要小于 Q(s1, a2)=1, 所以我们判断要选择 a2 作为下一个行为.

现在我们的状态更新成 s2 , 我们还是有两个同样的选择, 重复上面的过程, 在行为准则Q 表中寻找 Q(s2, a1) Q(s2, a2) 的值, 并比较他们的大小, 选取较大的一个. 

接着根据 a2 我们到达 s3 并在此重复上面的决策过程. Q learning 的方法也就是这样决策的. 看完决策, 我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改, 提升的.

## Q-Learning 更新

![image2024-7-11_21-15-52](https://s41.ax1x.com/2025/12/10/pZubGLQ.png)

所以我们回到之前的流程, 根据 Q 表的估计, 因为在 s1 中, a2 的值比较大, 通过之前的决策方法, 我们在 s1 采取了 a2, 并到达 s2,

这时我们开始更新用于决策的 Q 表, 接着我们并没有在实际中采取任何行为, 而是再想象自己在 s2 上采取了每种行为, 分别看看两种行为哪一个的 Q 值大, 比如说 Q(s2, a2) 的值比 Q(s2, a1) 的大, 所以我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励 R (这里还没有获取到我们的棒棒糖, 所以奖励为 0), 因为会获取实实在在的奖励 R , 我们将这个作为我现实中 Q(s1, a2) 的值

但是我们之前是根据 Q 表估计 Q(s1, a2) 的值. 所以有了现实和估计值, 我们就能更新Q(s1, a2) , 根据 估计与现实的差距, 将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值 变成新的值.

但时刻记住, 我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在 s2 做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做. 

这就是  Q learning 是如何决策和学习优化决策的过程.

![image2024-7-11_21-18-9](https://s41.ax1x.com/2025/12/10/pZubYZj.png)

这一张图概括了我们之前所有的内容. 这也是 Q learning 的算法, 每次更新我们都用到了 Q 现实和 Q 估计, 而且 Q learning 的迷人之处就是 在 Q(s1, a2) 现实 中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实, 很奇妙吧. 最后我们来说说这套算法中一些参数的意义. Epsilon greedy 是用在决策上的一种策略, 比如 epsilon = 0.9 时, 就说明有90% 的情况我会按照 Q 表的最优值选择行为, 10% 的时间使用随机选行为. alpha是学习率, 来决定这次的误差有多少是要被学习的, alpha是一个小于1 的数. gamma是折扣因子，决定了未来奖励的重要性，

# 什么是 DQN（ Deep Q Network）（大入门）

DQN是一种融合了神经网络和 Q learning 的方法

## 神经网络的作用

![image2024-7-11_21-21-15](https://s41.ax1x.com/2025/12/10/pZubtds.png)

我们使用表格来存储每一个状态 state, 和在这个 state 每个行为 action 所拥有的 Q 值.

而当今问题是在太复杂, 状态可以多到比天上的星星还多(比如下围棋，自动驾驶). 如果全用表格来存储它们, 恐怕我们的计算机有再大的内存都不够, 而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事. 

不过, 在机器学习中, 有一种方法对这种事情很在行, 那就是神经网络. 我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 这样我们就没必要在表格中记录 Q 值, 而是直接使用神经网络生成 Q 值.

还有一种形式的是这样, 我们也能只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作. 我们可以想象, 神经网络接受外部的信息, 相当于眼睛鼻子耳朵收集信息, 然后通过大脑加工输出每种动作的值, 最后通过强化学习的方式选择动作，这也是端到端模型的雏形。

![image2024-7-11_21-27-47](https://s41.ax1x.com/2025/12/10/pZubMJP.png)

DQN 有一个记忆库用于学习之前的经历, Q learning 是一种 off-policy 离线学习法, 它能学习当前经历着的, 也能学习过去经历过的, 甚至是学习别人的经历. 所以每次 DQN 更新的时候, 我们都可以随机抽取一些之前的经历进行学习.

随机抽取这种做法打乱了经历之间的相关性, 也使得神经网络更新更有效率. Fixed Q-targets 也是一种打乱相关性的机理, 如果使用 fixed Q-targets, 我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络, 预测 Q 估计 的神经网络具备最新的参数, 而预测 Q 现实 的神经网络使用的参数则是很久以前的. 有了这两种提升手段, DQN 才能在一些游戏中超越人类.

# 实践

[Colab实践链接](https://colab.research.google.com/drive/1RHCIkRPlcB_ERlwgUsCcNrt97kMLrRsP#scrollTo=4qBoFjFo4smJ)

# 参考文档

[Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)